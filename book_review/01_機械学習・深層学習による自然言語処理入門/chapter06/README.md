## メモ
### 疑問点などを記載していく
- P122 なんで負の対数尤度を考える必要があるか
    - 尤度関数の最大化=最尤推定
    ⇆ 対数尤度関数を最大化  
    ⇆ 対数尤度関数にマイナスを取ったものを最小化
    ⇆ 交差エントロピー（=対数尤度関数にマイナスを取ったもの）の最小化
- 確率と尤度
    - 確率: 正規分布を考えてみる
        - 平均と分散の2つのパラメータで分布が決まる
            - xを変数として変えていく
    - 尤度: 
        - データxがわかっていて、平均と分散の2つのパラメータが不明でこの2つが変数となっている
        - 「サンプリングしてデータが観測された後、そのデータは元々どのようなパラメータを持つ確率分布から生まれたものなのか？」という話
- モデルの重みを学習するための必要な要素
    - 損失関数
        - モデルの良さを評価する関数
            - 損失関数の値を小さくするようにモデルを学習していく
        - 損失関数の例:
            - 平均二乗誤差（MSE）
                - 予測と正解の差の二乗和をデータセット全体で平均する
            - クロスエントロピー（交差エントロピー）
                - 間違ったクラスに対して高い確率を予測するとlossが大きくなる
        - 対数尤度を考える理由
            - アンダーフローが起きるため
            - 微分しやすいため
    - オプティマイザー
        - モデルの重みを更新するアルゴリズム
        - モデル学習時には損失関数の値を最小化するために、重みの更新を行う。この重みの更新の仕方（動かし方）を決めるのがオプティマイザー
        - 勾配降下法
            1. 損失関数の勾配を計算
            2. 勾配に基づいて重みを更新
            3. 損失関数の値が変化しなくなるまで1と2を繰り返す
            - 重みの初期値はランダム
                - Heの初期値などいくつかある
            重みの更新式もいくつかある
            